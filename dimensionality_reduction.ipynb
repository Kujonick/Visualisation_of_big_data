{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcbd94b4-7169-4301-8078-c4efdf90d8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 19:55:16.942167: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-14 19:55:17.274993: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-06-14 19:55:17.275012: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-06-14 19:55:17.315737: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-14 19:55:18.224357: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-14 19:55:18.224452: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-06-14 19:55:18.224459: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1933116a-b248-4782-9041-ade409e07a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', encoding='windows-1252', header=None, usecols=[2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf1bf9b-6379-44de-88b6-b4823f0de221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wladek/.local/lib/python3.8/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "df['date'] = pd.to_datetime(df[2].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cecf012b-c564-4029-b77d-2e50e1ad7acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>2009-06-16 08:40:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    2  \\\n",
       "0        Mon Apr 06 22:19:45 PDT 2009   \n",
       "1        Mon Apr 06 22:19:49 PDT 2009   \n",
       "2        Mon Apr 06 22:19:53 PDT 2009   \n",
       "3        Mon Apr 06 22:19:57 PDT 2009   \n",
       "4        Mon Apr 06 22:19:57 PDT 2009   \n",
       "...                               ...   \n",
       "1599995  Tue Jun 16 08:40:49 PDT 2009   \n",
       "1599996  Tue Jun 16 08:40:49 PDT 2009   \n",
       "1599997  Tue Jun 16 08:40:49 PDT 2009   \n",
       "1599998  Tue Jun 16 08:40:49 PDT 2009   \n",
       "1599999  Tue Jun 16 08:40:50 PDT 2009   \n",
       "\n",
       "                                                         5                date  \n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t... 2009-04-06 22:19:45  \n",
       "1        is upset that he can't update his Facebook by ... 2009-04-06 22:19:49  \n",
       "2        @Kenichan I dived many times for the ball. Man... 2009-04-06 22:19:53  \n",
       "3          my whole body feels itchy and like its on fire  2009-04-06 22:19:57  \n",
       "4        @nationwideclass no, it's not behaving at all.... 2009-04-06 22:19:57  \n",
       "...                                                    ...                 ...  \n",
       "1599995  Just woke up. Having no school is the best fee... 2009-06-16 08:40:49  \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv... 2009-06-16 08:40:49  \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f... 2009-06-16 08:40:49  \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ... 2009-06-16 08:40:49  \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity... 2009-06-16 08:40:50  \n",
       "\n",
       "[1600000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15a172c-072a-4106-898d-eaed182c4ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wladek/.local/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Setup pipeline on GPU\n",
    "fe_pipeline = pipeline(\n",
    "    task=\"feature-extraction\",\n",
    "    model=\"vinai/bertweet-base\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e198d7b7-4d97-4ae9-8527-b477e3d77758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/50000 [00:00<?, ?it/s]/home/wladek/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  \n",
      "/tmp/ipykernel_127459/1486891721.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(outputs).float().cpu().numpy() # 32 x 768\n",
      "100%|███████████████████████████████████| 50000/50000 [7:19:05<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_pandas(df[[5]])\n",
    "batch_size=32\n",
    "texts = df[5].tolist()\n",
    "output_file = \"/media/wladek/Wladyslaw/bertweet_embeddings.pt\"\n",
    "with open(output_file, \"wb\") as f:\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            outputs_list = fe_pipeline(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            outputs = torch.stack([emb[0, 0, :] for emb in outputs_list])\n",
    "\n",
    "        emb = torch.tensor(outputs).float().cpu().numpy() # 32 x 768\n",
    "        f.write(emb.tobytes())  # write raw bytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d92260a0-d34e-499e-9fe6-62d6ab58f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open(\"/media/wladek/Wladyslaw/bertweet_embeddings.pt\", \"rb\") as f:\n",
    "    # header = np.frombuffer(f.read(8), dtype=np.int32)\n",
    "    # total_embeddings, embedding_dim = header\n",
    "    data = f.read()\n",
    "    embeddings = np.frombuffer(data, dtype=np.float32).reshape(1_600_000, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf764fe-4341-4c41-ae97-5d32d82d4c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55f0a8a2-97ee-4670-8e92-5a7db55fad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice([i for i in range(1_600_000)], size=16_000, replace=False)\n",
    "embeddings_train = embeddings[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b82bbcaa-bcf2-474b-9f5d-84652d0044ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5b43301-12a1-4d81-87b7-229f550826bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "import joblib\n",
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=5,        # controls local vs global structure\n",
    "    n_components=3,        # 2D for visualization; use 3 or more if needed\n",
    "    metric='cosine',       # or 'euclidean'\n",
    "    n_jobs=-1\n",
    ").fit(embeddings_train)\n",
    "result = umap_model.transform(embeddings)\n",
    "result.dump(\"umap_result_3d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13bbef25-6ce4-4bdb-bc92-a95180aee313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = np.load(\"umap_result_3d.npy\", allow_pickle=True)\n",
    "result1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7c05af3-8eef-4f7f-8460-e999cfc54436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pacmap import PaCMAP\n",
    "pacmap_model = PaCMAP(n_components=2,\n",
    "                      n_neighbors=10,\n",
    "                      MN_ratio=0.5,\n",
    "                      FP_ratio=2.0,\n",
    "                     save_tree=True)\n",
    "pacmap_model.fit(embeddings_train)\n",
    "result = pacmap_model.transform(embeddings)\n",
    "result.dump(\"pacmap_result.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f97d9b4c-5566-4ded-a8f0-0e6df1186065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = np.load(\"pacmap_result.npy\", allow_pickle=True)\n",
    "result1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7e3d763-3450-4dd9-9a6b-d56876cb2263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: `n_components != 2` have not been thoroughly tested.\n"
     ]
    }
   ],
   "source": [
    "pacmap_model = PaCMAP(n_components=3,\n",
    "                      n_neighbors=10,\n",
    "                      MN_ratio=0.5,\n",
    "                      FP_ratio=2.0,\n",
    "                     save_tree=True)\n",
    "pacmap_model.fit(embeddings_train)\n",
    "result = pacmap_model.transform(embeddings)\n",
    "result.dump(\"pacmap_result_3d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a351720d-8506-439c-931f-bdd468836241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = np.load(\"pacmap_result_3d.npy\", allow_pickle=True)\n",
    "result1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "448ce2d4-354a-4b97-9bf5-8ec7f560a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_model = PCA(n_components=2)\n",
    "pca_model.fit(embeddings_train)\n",
    "result = pca_model.transform(embeddings)\n",
    "result.dump(\"pca_result.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2378f58-5323-4018-bd74-e2019704e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=3)\n",
    "pca_model.fit(embeddings_train)\n",
    "result = pca_model.transform(embeddings)\n",
    "result.dump(\"pca_result_3d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a46b318a-a245-4bbb-a7f0-91590440a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "pca_model = IncrementalPCA(n_components=2, batch_size=1000)\n",
    "pca_model.fit(embeddings)\n",
    "result = pca_model.transform(embeddings)\n",
    "result.dump(\"incremental_pca_result.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e90a196-3a6d-4d58-87e7-8c1685b600ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "pca_model = IncrementalPCA(n_components=3, batch_size=1000)\n",
    "pca_model.fit(embeddings)\n",
    "result = pca_model.transform(embeddings)\n",
    "result.dump(\"incremental_pca_result_3d.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c1547-b6e6-4cf6-9b39-f59126071727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
