{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd94b4-7169-4301-8078-c4efdf90d8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Install\\anaconda\\envs\\wdzd_project\\lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Install\\anaconda\\envs\\wdzd_project\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1933116a-b248-4782-9041-ade409e07a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('data\\original.csv', encoding='windows-1252', header=None, usecols=[2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf1bf9b-6379-44de-88b6-b4823f0de221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Przemek\\AppData\\Local\\Temp\\ipykernel_5212\\146429819.py:1: FutureWarning: Parsed string \"Mon Apr 06 22:19:45 PDT 2009\" included an un-recognized timezone \"PDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  df_raw['date'] = pd.to_datetime(df_raw[2].astype(str))\n"
     ]
    }
   ],
   "source": [
    "df_raw['date'] = pd.to_datetime(df_raw[2].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf012b-c564-4029-b77d-2e50e1ad7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.drop(columns=[2]).rename(columns={5: 'tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0570017e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet            object\n",
       "date     datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_parquet('data\\\\refined.parquet')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15a172c-072a-4106-898d-eaed182c4ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Install\\anaconda\\envs\\wdzd_project\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Install\\anaconda\\envs\\wdzd_project\\lib\\site-packages\\transformers\\modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "# Setup pipeline on GPU\n",
    "fe_pipeline = pipeline(\n",
    "    task=\"feature-extraction\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device='cuda:0',\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e198d7b7-4d97-4ae9-8527-b477e3d77758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]C:\\Users\\Przemek\\AppData\\Local\\Temp\\ipykernel_5212\\738702710.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(outputs).float().cpu().numpy() # 32 x 768\n",
      " 31%|███▏      | 10/32 [00:05<00:08,  2.50it/s]c:\\Install\\anaconda\\envs\\wdzd_project\\lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 32/32 [00:13<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "ds = Dataset.from_pandas(df[['tweet']])\n",
    "batch_size=32\n",
    "texts = df['tweet'].tolist()\n",
    "output_file = \"D:\\\\WDZD\\\\bertweet_embeddings.pt\"\n",
    "\n",
    "examples_count = 1000# len(texts)\n",
    "\n",
    "with open(output_file, \"wb\") as f:\n",
    "    for i in tqdm(range(0, examples_count, batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            outputs_list = fe_pipeline(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            outputs = torch.stack([emb[0, 0, :] for emb in outputs_list])\n",
    "\n",
    "        emb = torch.tensor(outputs).float().cpu().numpy() # 32 x 768\n",
    "        f.write(emb.tobytes())  # write raw bytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d92260a0-d34e-499e-9fe6-62d6ab58f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open(output_file, \"rb\") as f:\n",
    "    # header = np.frombuffer(f.read(8), dtype=np.int32)\n",
    "    # total_embeddings, embedding_dim = header\n",
    "    data = f.read()\n",
    "    embeddings = np.frombuffer(data, dtype=np.float32).reshape(int(np.ceil(examples_count / 32) * 32), 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f886946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c9fb92-4f0c-4bdb-ade2-a97a3290b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by='date')\n",
    "df['time_group'] = pd.qcut(df_sorted['date'].rank(method='first'), q=100, labels=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faff9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:int(np.ceil(examples_count / 32) * 32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee3ca3-2ed9-4310-9fad-00aa4761a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grouped_embeddings = {}\n",
    "for group in range(100):\n",
    "    indices = df[df['time_group'] == group].index\n",
    "    time_grouped_embeddings[group] = embeddings[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f966c-7623-47d5-97cb-12a3e17d98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_grouped_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7f6f4-2c6d-44c0-a8bf-1bb165f4e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grouped_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b43301-12a1-4d81-87b7-229f550826bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Assuming your embeddings are in this variable\n",
    "# embeddings.shape == (1600000, 768)\n",
    "umaps = dict()\n",
    "for key in time_grouped_embeddings:\n",
    "    if time_grouped_embeddings[key].shape[0] == 0:\n",
    "        continue\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=5,        # controls local vs global structure\n",
    "        n_components=2,        # 2D for visualization; use 3 or more if needed\n",
    "        metric='cosine',       # or 'euclidean'\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    embedding_2d = umap_model.fit_transform(time_grouped_embeddings[key])\n",
    "    joblib.dump(umap_model, f\"models/umap_model_{key}.pkl\")\n",
    "    print(\"UMAP output shape:\", embedding_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3d763-3450-4dd9-9a6b-d56876cb2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap0 = joblib.load(f\"models/umap_model_{0}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351720d-8506-439c-931f-bdd468836241",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fe_pipeline(\"I like trains\")[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ce2d4-354a-4b97-9bf5-8ec7f560a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.tensor(fe_pipeline(\"I like trains\")[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2378f58-5323-4018-bd74-e2019704e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap0.transform(embedding.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b318a-a245-4bbb-a7f0-91590440a9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wdzd_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
